# Apache Airflow for ResearcherAI

Production-grade ETL pipeline orchestration for automated research paper collection and processing.

## ğŸ¯ What This Does

Replaces the simple scheduler with Apache Airflow to provide:
- **Parallel data collection** from 7 sources (3-4x faster)
- **Visual workflow monitoring** via web UI
- **Automatic retries** with exponential backoff
- **Health monitoring** and alerting
- **DAG-based workflows** with task dependencies
- **Scalable execution** with Celery workers

## ğŸ“ Directory Structure

```
airflow/
â”œâ”€â”€ docker-compose.yml      # Infrastructure setup
â”œâ”€â”€ .env                     # Environment configuration
â”œâ”€â”€ setup_airflow.sh         # Setup and start script
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ dags/                    # DAG definitions
â”‚   â”œâ”€â”€ research_paper_etl.py      # Main ETL pipeline
â”‚   â””â”€â”€ system_monitoring.py       # Health checks
â”œâ”€â”€ logs/                    # Task execution logs
â”œâ”€â”€ plugins/                 # Custom operators (optional)
â””â”€â”€ config/                  # Airflow config overrides
```

## ğŸš€ Quick Start

```bash
# 1. Navigate to airflow directory
cd airflow

# 2. Run setup script
./setup_airflow.sh

# 3. Access UI
# Airflow UI: http://localhost:8080 (airflow/airflow)
# Flower UI: http://localhost:5555
```

## ğŸ“Š DAGs Included

### 1. research_paper_etl
**Schedule**: Every 6 hours
**Purpose**: Collect research papers from multiple sources in parallel

**Workflow**:
```
collect (parallel) â†’ merge â†’ check_threshold â†’ process â†’ summary
```

**Tasks**:
- Parallel collection from 7 sources
- Merge and deduplicate papers
- Threshold check (min 10 papers)
- Store in vector DB (FAISS)
- Extract knowledge triples
- Generate run summary

### 2. system_monitoring
**Schedule**: Every 30 minutes
**Purpose**: Monitor system health and alert on issues

**Checks**:
- API health
- Vector DB status
- Graph DB status
- Disk space
- Memory usage

## ğŸ”§ Configuration

### Environment Variables (.env)
```bash
GOOGLE_API_KEY=your-api-key
RESEARCHER_AI_SESSION=airflow_default
RESEARCHER_AI_MAX_PAPERS_PER_SOURCE=10
```

### Airflow Variables (via UI)
- `research_query`: Search query for paper collection
- `max_papers_per_source`: Papers per source
- `min_papers_threshold`: Minimum for processing
- `session_name`: ResearcherAI session name

## ğŸ“ˆ Performance Comparison

| Metric | Before (Sequential) | After (Airflow) | Improvement |
|--------|-------------------|-----------------|-------------|
| Collection Time | 19-38s | 5-10s | **3-4x faster** |
| Retry Logic | None | 3 retries | **Robust** |
| Monitoring | Logs only | Full UI | **Visual** |
| Parallelism | Sequential | 7 parallel | **7x concurrent** |
| Error Recovery | Manual | Automatic | **Resilient** |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Airflow Ecosystem                 â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚ Web UI   â”‚ (localhost:8080)             â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚       â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚  PostgreSQL   â”‚  Metadata DB            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚       â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚  Scheduler    â”‚  Triggers tasks         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚       â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚  Redis Queue  â”‚  Task queue             â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚       â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ Workers (3x)  â”‚  Execute tasks          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Common Operations

### Start Services
```bash
./setup_airflow.sh
```

### Stop Services
```bash
docker compose down
```

### View Logs
```bash
docker compose logs -f airflow-scheduler
docker compose logs -f airflow-worker
```

### Trigger DAG Manually
```bash
docker compose exec airflow-webserver airflow dags trigger research_paper_etl
```

### Access Airflow CLI
```bash
docker compose exec airflow-webserver bash
```

## ğŸ” Monitoring

### Airflow UI (Port 8080)
- View DAG runs and task status
- Check logs and execution times
- Monitor historical performance
- Manage variables and connections

### Flower UI (Port 5555)
- Monitor Celery workers
- View task queue
- Check worker performance
- Track task execution

## ğŸ› Troubleshooting

### DAGs Not Appearing
```bash
# Check for syntax errors
docker compose exec airflow-webserver python /opt/airflow/dags/research_paper_etl.py

# Check scheduler logs
docker compose logs airflow-scheduler
```

### Tasks Failing
1. Click on failed task in UI
2. View logs
3. Check for API timeouts or import errors
4. Adjust `execution_timeout` if needed

### Reset Everything
```bash
docker compose down -v
./setup_airflow.sh
```

## ğŸ“š Documentation

- **Concepts**: `../AIRFLOW_COMPLETE_GUIDE.md`
- **Usage**: `../AIRFLOW_USAGE_GUIDE.md`
- **Official Docs**: https://airflow.apache.org/docs/

## ğŸ“ Next Steps

1. **Customize queries**: Edit Airflow Variables
2. **Add sources**: Extend `research_paper_etl.py`
3. **Configure alerts**: Set up email/Slack notifications
4. **Scale workers**: Adjust replicas in `docker-compose.yml`
5. **Monitor performance**: Use Gantt charts in UI

## ğŸ“Š System Requirements

- Docker: 20.10+
- Docker Compose: 2.0+
- RAM: 4GB minimum (8GB recommended)
- Disk: 10GB free space
- CPU: 2+ cores

## ğŸš€ Production Checklist

- [ ] Set strong passwords in `.env`
- [ ] Configure email alerts (SMTP)
- [ ] Set up external database (not SQLite)
- [ ] Configure persistent volumes
- [ ] Enable HTTPS for web UI
- [ ] Set up monitoring (Prometheus/Grafana)
- [ ] Configure backup strategy
- [ ] Set resource limits in Docker

## ğŸ†˜ Support

- Issues: https://github.com/Sebuliba-Adrian/ResearcherAI/issues
- Airflow Community: https://airflow.apache.org/community/

---

**Powered by Apache Airflow 2.10.2** | **Built for ResearcherAI**
