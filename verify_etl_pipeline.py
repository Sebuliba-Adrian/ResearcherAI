#!/usr/bin/env python3
"""
ETL Pipeline Deep Verification - End-to-End
===========================================
This script verifies the COMPLETE ETL pipeline:
1. Collect real papers from sources
2. Insert into Neo4j (verify with queries)
3. Insert into Qdrant (verify with queries)
4. Compare data consistency between source â†’ Neo4j â†’ Qdrant
"""

import os
import sys
import json
import time
from datetime import datetime

print("=" * 100)
print("ğŸ” DEEP ETL PIPELINE VERIFICATION - End-to-End")
print("=" * 100)
print(f"Started: {datetime.now().isoformat()}\n")

# Setup
os.environ["GOOGLE_API_KEY"] = "AIzaSyCGUWaN4uzBBrnXFZ_qWBqKaeSVa13Lip4"
os.environ["CUDA_VISIBLE_DEVICES"] = ""

# ============================================================================
# STEP 1: COLLECT REAL PAPERS
# ============================================================================

print("\n" + "=" * 100)
print("STEP 1: Collect Real Papers from Sources")
print("=" * 100)

from agents.data_agent import DataCollectorAgent

collector = DataCollectorAgent()
query = "attention mechanism neural networks"
max_papers = 5

print(f"\nCollecting {max_papers} papers for query: '{query}'")
print("Sources: arXiv, Semantic Scholar, Zenodo, PubMed, Web")

start_time = time.time()
collected_papers = collector.collect_all(query, max_per_source=max_papers)
collection_time = time.time() - start_time

print(f"\nâœ… Collection Complete!")
print(f"   Papers collected: {len(collected_papers)}")
print(f"   Time taken: {collection_time:.2f}s")

# Show sample papers
print(f"\nğŸ“„ Sample Papers Collected:")
for i, paper in enumerate(collected_papers[:3], 1):
    print(f"\n{i}. {paper.get('title', 'N/A')[:70]}...")
    print(f"   Source: {paper.get('source', 'N/A')}")
    print(f"   Authors: {len(paper.get('authors', []))} authors")
    print(f"   Abstract: {paper.get('abstract', 'N/A')[:100]}...")

# Save collected papers for comparison
with open('./test_outputs/etl_source_papers.json', 'w') as f:
    json.dump(collected_papers, f, indent=2, default=str)

print(f"\nğŸ’¾ Source papers saved to: ./test_outputs/etl_source_papers.json")

# ============================================================================
# STEP 2: INSERT INTO NEO4J AND VERIFY
# ============================================================================

print("\n\n" + "=" * 100)
print("STEP 2: Insert into Neo4j and Verify")
print("=" * 100)

from neo4j import GraphDatabase
from agents.graph_agent import KnowledgeGraphAgent

# Initialize Neo4j
neo4j_config = {
    "type": "neo4j",
    "uri": "bolt://localhost:7687",
    "user": "neo4j",
    "password": "research_password"
}

print(f"\nğŸ”Œ Connecting to Neo4j...")
driver = GraphDatabase.driver(
    neo4j_config["uri"],
    auth=(neo4j_config["user"], neo4j_config["password"])
)

# Clear previous data for clean test
print(f"ğŸ—‘ï¸  Clearing previous test data...")
with driver.session() as session:
    session.run("MATCH (n:TestPaper) DETACH DELETE n")

print(f"âœ… Neo4j connected and ready")

# Initialize Graph Agent
graph_agent = KnowledgeGraphAgent(config=neo4j_config)

# Insert papers
print(f"\nğŸ“¥ Inserting {len(collected_papers)} papers into Neo4j...")
start_time = time.time()

# Mark papers as test papers
for paper in collected_papers:
    paper['test_marker'] = 'etl_verification_test'

graph_stats = graph_agent.process_papers(collected_papers)
insert_time = time.time() - start_time

print(f"\nâœ… Neo4j Insertion Complete!")
print(f"   Time taken: {insert_time:.2f}s")
print(f"   Graph stats: {graph_stats}")

# VERIFY: Query Neo4j directly to confirm data
print(f"\nğŸ” VERIFICATION: Querying Neo4j directly...")

with driver.session() as session:
    # Count total papers
    result = session.run("MATCH (p:Paper) RETURN count(p) as count")
    total_papers_in_neo4j = result.single()["count"]

    # Get papers we just inserted
    result = session.run("""
        MATCH (p:Paper)
        WHERE p.title IN $titles
        RETURN p.title as title, p.source as source, p.abstract as abstract
        LIMIT 5
    """, titles=[p['title'] for p in collected_papers[:5]])

    neo4j_papers = [dict(record) for record in result]

    # Get relationships
    result = session.run("""
        MATCH (p1:Paper)-[r]->(p2:Paper)
        WHERE p1.title IN $titles OR p2.title IN $titles
        RETURN p1.title as from_paper, type(r) as rel_type, p2.title as to_paper
        LIMIT 10
    """, titles=[p['title'] for p in collected_papers[:5]])

    neo4j_relationships = [dict(record) for record in result]

print(f"\nğŸ“Š Neo4j Verification Results:")
print(f"   Total papers in database: {total_papers_in_neo4j}")
print(f"   Papers retrieved matching our collection: {len(neo4j_papers)}")
print(f"   Relationships found: {len(neo4j_relationships)}")

print(f"\nğŸ“„ Sample Papers in Neo4j:")
for i, paper in enumerate(neo4j_papers[:3], 1):
    print(f"\n{i}. {paper['title'][:70]}...")
    print(f"   Source: {paper['source']}")
    print(f"   Abstract: {paper['abstract'][:100] if paper['abstract'] else 'N/A'}...")

# Save Neo4j results
neo4j_verification = {
    "total_papers": total_papers_in_neo4j,
    "papers_retrieved": neo4j_papers,
    "relationships": neo4j_relationships,
    "insertion_time": insert_time
}

with open('./test_outputs/etl_neo4j_verification.json', 'w') as f:
    json.dump(neo4j_verification, f, indent=2, default=str)

print(f"\nğŸ’¾ Neo4j results saved to: ./test_outputs/etl_neo4j_verification.json")

# ============================================================================
# STEP 3: INSERT INTO QDRANT AND VERIFY
# ============================================================================

print("\n\n" + "=" * 100)
print("STEP 3: Insert into Qdrant and Verify")
print("=" * 100)

from qdrant_client import QdrantClient
from agents.vector_agent import VectorAgent

# Initialize Qdrant
qdrant_config = {
    "type": "qdrant",
    "host": "localhost",
    "port": 6333
}

print(f"\nğŸ”Œ Connecting to Qdrant...")
qdrant_client = QdrantClient(host="localhost", port=6333)

# Check if collection exists
collections = qdrant_client.get_collections()
collection_name = "research_papers"

collection_exists = any(c.name == collection_name for c in collections.collections)
print(f"âœ… Qdrant connected. Collection '{collection_name}' exists: {collection_exists}")

# Initialize Vector Agent
vector_agent = VectorAgent(config=qdrant_config)

# Get count before insertion
collection_info = qdrant_client.get_collection(collection_name)
vectors_before = collection_info.points_count

print(f"\nğŸ“Š Vectors in collection before insertion: {vectors_before}")

# Insert papers
print(f"\nğŸ“¥ Inserting {len(collected_papers)} papers into Qdrant...")
start_time = time.time()
vector_stats = vector_agent.process_papers(collected_papers)
vector_insert_time = time.time() - start_time

print(f"\nâœ… Qdrant Insertion Complete!")
print(f"   Time taken: {vector_insert_time:.2f}s")
print(f"   Vector stats: {vector_stats}")

# Get count after insertion
collection_info = qdrant_client.get_collection(collection_name)
vectors_after = collection_info.points_count

print(f"\nğŸ“Š Vectors in collection after insertion: {vectors_after}")
print(f"   New vectors added: {vectors_after - vectors_before}")

# VERIFY: Search Qdrant to confirm data
print(f"\nğŸ” VERIFICATION: Semantic search in Qdrant...")

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')

# Search for papers about attention
search_query = "attention mechanism transformers"
query_vector = model.encode(search_query).tolist()

search_results = qdrant_client.search(
    collection_name=collection_name,
    query_vector=query_vector,
    limit=5
)

print(f"\nğŸ” Search Results for '{search_query}':")
for i, result in enumerate(search_results[:5], 1):
    print(f"\n{i}. Score: {result.score:.4f}")
    print(f"   Title: {result.payload.get('title', 'N/A')[:70]}...")
    print(f"   Source: {result.payload.get('source', 'N/A')}")

# Save Qdrant results
qdrant_verification = {
    "vectors_before": vectors_before,
    "vectors_after": vectors_after,
    "new_vectors": vectors_after - vectors_before,
    "search_results": [
        {
            "score": r.score,
            "title": r.payload.get('title'),
            "source": r.payload.get('source'),
            "year": r.payload.get('year')
        } for r in search_results
    ],
    "insertion_time": vector_insert_time
}

with open('./test_outputs/etl_qdrant_verification.json', 'w') as f:
    json.dump(qdrant_verification, f, indent=2, default=str)

print(f"\nğŸ’¾ Qdrant results saved to: ./test_outputs/etl_qdrant_verification.json")

# ============================================================================
# STEP 4: DATA CONSISTENCY VERIFICATION
# ============================================================================

print("\n\n" + "=" * 100)
print("STEP 4: Data Consistency Verification")
print("=" * 100)

print(f"\nğŸ” Comparing data across the pipeline...")

# Compare source papers vs Neo4j
source_titles = set([p['title'] for p in collected_papers])
neo4j_titles = set([p['title'] for p in neo4j_papers])

title_match_rate = len(neo4j_titles.intersection(source_titles)) / len(source_titles) if source_titles else 0

print(f"\nğŸ“Š Source â†’ Neo4j Consistency:")
print(f"   Source papers: {len(source_titles)}")
print(f"   Neo4j papers retrieved: {len(neo4j_titles)}")
print(f"   Title match rate: {title_match_rate * 100:.1f}%")

# Check if abstracts are preserved
abstract_match = 0
for source_paper in collected_papers[:5]:
    for neo4j_paper in neo4j_papers:
        if source_paper['title'] == neo4j_paper['title']:
            source_abstract = source_paper.get('abstract', '')[:100]
            neo4j_abstract = neo4j_paper.get('abstract', '')[:100] if neo4j_paper.get('abstract') else ''
            if source_abstract and neo4j_abstract and source_abstract in neo4j_abstract:
                abstract_match += 1

print(f"   Abstract preservation: {abstract_match}/{min(len(collected_papers), 5)} verified")

# Compare with Qdrant
qdrant_titles = set([r.payload.get('title') for r in search_results])
qdrant_match_rate = len(qdrant_titles.intersection(source_titles)) / len(source_titles) if source_titles else 0

print(f"\nğŸ“Š Source â†’ Qdrant Consistency:")
print(f"   Source papers: {len(source_titles)}")
print(f"   Qdrant papers in search: {len(qdrant_titles)}")
print(f"   Papers found in search: {len(qdrant_titles.intersection(source_titles))}")
print(f"   Note: Qdrant shows top semantic matches, not all papers")

# Overall pipeline check
consistency_report = {
    "source_to_neo4j": {
        "source_count": len(source_titles),
        "neo4j_count": len(neo4j_titles),
        "match_rate": title_match_rate,
        "status": "âœ… PASS" if title_match_rate >= 0.8 else "âŒ FAIL"
    },
    "neo4j_to_qdrant": {
        "neo4j_papers": len(neo4j_papers),
        "vectors_added": vectors_after - vectors_before,
        "status": "âœ… PASS" if (vectors_after - vectors_before) > 0 else "âŒ FAIL"
    },
    "data_integrity": {
        "abstract_preservation": f"{abstract_match}/{min(len(collected_papers), 5)}",
        "status": "âœ… PASS" if abstract_match >= 3 else "âŒ FAIL"
    }
}

with open('./test_outputs/etl_consistency_report.json', 'w') as f:
    json.dump(consistency_report, f, indent=2)

# ============================================================================
# STEP 5: END-TO-END QUERY VERIFICATION
# ============================================================================

print("\n\n" + "=" * 100)
print("STEP 5: End-to-End Query Verification")
print("=" * 100)

print(f"\nğŸ” Testing complete query flow: Qdrant â†’ Neo4j integration...")

# Get top results from Qdrant
top_paper_from_qdrant = search_results[0].payload if search_results else None

if top_paper_from_qdrant:
    print(f"\nğŸ“„ Top paper from Qdrant semantic search:")
    print(f"   Title: {top_paper_from_qdrant.get('title', 'N/A')[:70]}...")
    print(f"   Score: {search_results[0].score:.4f}")

    # Now find this paper in Neo4j and get its relationships
    with driver.session() as session:
        result = session.run("""
            MATCH (p:Paper {title: $title})
            OPTIONAL MATCH (p)-[r]-(related:Paper)
            RETURN p.title as title, p.source as source,
                   collect(DISTINCT related.title) as related_papers,
                   count(DISTINCT r) as relationship_count
        """, title=top_paper_from_qdrant.get('title'))

        neo4j_context = result.single()

        if neo4j_context:
            print(f"\nğŸ”— Neo4j Graph Context for this paper:")
            print(f"   Related papers: {neo4j_context['relationship_count']}")
            if neo4j_context['related_papers']:
                for related in neo4j_context['related_papers'][:3]:
                    print(f"      - {related[:60]}...")
        else:
            print(f"\nâš ï¸  Paper not found in Neo4j graph")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print("\n\n" + "=" * 100)
print("FINAL ETL PIPELINE VERIFICATION SUMMARY")
print("=" * 100)

summary = {
    "collection": {
        "papers_collected": len(collected_papers),
        "time": f"{collection_time:.2f}s",
        "status": "âœ… PASS"
    },
    "neo4j": {
        "papers_inserted": len(neo4j_papers),
        "relationships_created": len(neo4j_relationships),
        "time": f"{insert_time:.2f}s",
        "match_rate": f"{title_match_rate * 100:.1f}%",
        "status": "âœ… PASS" if title_match_rate >= 0.8 else "âŒ FAIL"
    },
    "qdrant": {
        "vectors_added": vectors_after - vectors_before,
        "time": f"{vector_insert_time:.2f}s",
        "search_working": len(search_results) > 0,
        "status": "âœ… PASS" if (vectors_after - vectors_before) > 0 else "âŒ FAIL"
    },
    "consistency": consistency_report,
    "end_to_end": {
        "qdrant_neo4j_integration": neo4j_context is not None if top_paper_from_qdrant else False,
        "status": "âœ… PASS" if (top_paper_from_qdrant and neo4j_context) else "âš ï¸  PARTIAL"
    }
}

print(f"\nğŸ“Š Collection: {summary['collection']['status']}")
print(f"   Papers: {summary['collection']['papers_collected']}")
print(f"   Time: {summary['collection']['time']}")

print(f"\nğŸ“Š Neo4j: {summary['neo4j']['status']}")
print(f"   Papers: {summary['neo4j']['papers_inserted']}")
print(f"   Relationships: {summary['neo4j']['relationships_created']}")
print(f"   Match Rate: {summary['neo4j']['match_rate']}")

print(f"\nğŸ“Š Qdrant: {summary['qdrant']['status']}")
print(f"   Vectors Added: {summary['qdrant']['vectors_added']}")
print(f"   Search Working: {summary['qdrant']['search_working']}")

print(f"\nğŸ“Š End-to-End: {summary['end_to_end']['status']}")

with open('./test_outputs/etl_pipeline_summary.json', 'w') as f:
    json.dump(summary, f, indent=2, default=str)

print(f"\nğŸ’¾ Complete summary saved to: ./test_outputs/etl_pipeline_summary.json")

# Check overall pass
all_passed = (
    summary['collection']['status'] == "âœ… PASS" and
    summary['neo4j']['status'] == "âœ… PASS" and
    summary['qdrant']['status'] == "âœ… PASS"
)

print(f"\n\n{'=' * 100}")
if all_passed:
    print("ğŸ‰ ETL PIPELINE FULLY VERIFIED AND WORKING!")
    print("=" * 100)
    print("\nâœ… Source â†’ Neo4j: Working")
    print("âœ… Source â†’ Qdrant: Working")
    print("âœ… Data Consistency: Verified")
    print("âœ… End-to-End Query: Working")
    sys.exit(0)
else:
    print("âš ï¸  ETL PIPELINE HAS ISSUES")
    print("=" * 100)
    print("\nPlease review the detailed output above.")
    sys.exit(1)
