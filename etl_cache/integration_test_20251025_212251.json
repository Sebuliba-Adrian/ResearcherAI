{
  "loaded_at": "2025-10-25T21:22:51.876088",
  "count": 2,
  "data": [
    {
      "id": "arxiv_2510.20819v1",
      "title": "Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge",
      "abstract": "Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https:sites.google.comviewlddbmhome.",
      "authors": [
        "Nimrod Berman",
        "Omkar Joglekar",
        "Eitan Kosman",
        "Dotan Di Castro",
        "Omri Azencot"
      ],
      "topics": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arXiv",
      "url": "http://arxiv.org/abs/2510.20819v1",
      "published": "2025-10-23T17:59:54",
      "etl_processed": "2025-10-25T21:22:51.873384",
      "etl_source": "test_source",
      "etl_pipeline_version": "1.0",
      "text": "Title: Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge\n\nAbstract: Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https:sites.google.comviewlddbmhome.",
      "validated": true
    },
    {
      "id": "arxiv_2510.20818v1",
      "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
      "abstract": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robots physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https:vamos-vla.github.io",
      "authors": [
        "Mateo Guaman Castro",
        "Sidharth Rajagopal",
        "Daniel Gorbatov",
        "Matt Schmittle",
        "Rohan Baijal",
        "Octi Zhang",
        "Rosario Scalise",
        "Sidharth Talia",
        "Emma Romig",
        "Celso de Melo",
        "Byron Boots",
        "Abhishek Gupta"
      ],
      "topics": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arXiv",
      "url": "http://arxiv.org/abs/2510.20818v1",
      "published": "2025-10-23T17:59:45",
      "etl_processed": "2025-10-25T21:22:51.875092",
      "etl_source": "test_source",
      "etl_pipeline_version": "1.0",
      "text": "Title: VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation\n\nAbstract: A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robots physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https:vamos-vla.github.io",
      "validated": true
    }
  ]
}